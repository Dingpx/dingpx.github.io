<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Pengxiang Ding</title>
</head>
<body>
<div id="layout-content">
<div id="toptitle">
<h1>Pengxiang Ding</h1>
</div>
<table class="imgtable"><tr>
  <td><img src="pxding.jpg" alt="alt text" width="140px" /></td>

<td align="left">
  <p>M.Sc. Student<br />
School of Artificial Intelligence <br />
<a href="https://www.bupt.edu.cn/">Beijing University of Posts and Telecommunications(BUPT)</a><br />
<b>Phone</b>: +86 13699297699 <br /> 
<b>Email</b>: dingpx2015@gmail.com <br />
<b>Reasearch Interests</b>:  Human Motion Prediction, Human Pose Estimation, Human Motion Reconstruction, Self-supervised Learning, Uncertainty Learning. <br />
<br />
<!-- <a href="pdf/Aikun_Xu_CV.pdf">[CV]</a>   -->
  <a href="https://github.com/Dingpx">[GitHub]</a> <a href="https://scholar.google.com/citations?user=QyBSTzEAAAAJ&hl=zh-CN&oi=sra">[Google Scholar]</a> </p>
</td></tr></table>

<h2>Educations</h2>
<table class="imgtable"><tr><td>
<a href="https://www.bupt.edu.cn/"><img src="bupt_logo.png" alt="BUPT" width="80px" /></a>&nbsp;</td>
<td align="left"><h3>M. Sc., <a href="https://www.bupt.edu.cn/">BUPT </a> [2019.9 ~ 2022.7]</h3>
<ul>
<!-- <li><p>List of “Double First-Class” University Project、Project 985、Project 211</p>
</li> -->
<li><p><b>Major</b>: Control science and Engineering, School of Artificial Intelligence</p>
</li>
<li><p><b>GPA</b>: 3.71/4.0 <a href="Transcript_Ma.pdf">[Transcript]</a> </p>
</li>
<li><p><b>Awards</b>: 
  Beijing Outstanding Graduates [Top 5%] (2022), <br />
  Academic Scholarship[Top 15%](2020-2022) <br /></p>
  7th in ICCV2021 MMVRAC Skeleton-based Action Recognition [Top 5%](2021),<br />

</li>
</ul>
</td></tr></table>
<table class="imgtable"><tr><td>
<a href="https://www.bupt.edu.cn/"><img src="bupt_logo.png" alt="BUPT" width="80px" /></a>&nbsp;</td>
<td align="left"><h3>B. Sc., <a href="https://www.bupt.edu.cn/">BUPT </a>[2015.9 ~ 2019.7]</h3>
<ul>
<li><p><b>Major</b>: Logistics Engineering, School of Automation</p>
</li>
<li><p><b>GPA</b>: 3.51/4.0<a href="Transcript_Ba.pdf">[Transcript]</a></p>
</li>
<li><p><b>Awards</b>: 
1st Prize in Beijing university student Logistics Design Competition [Top 1%](2017), <br />
Honorable Mention in American College Students Mathematical Modeling Competition (2018), <br />
Academic Scholarship[Top 15%](2016-2019) <br />
<br /></p>
</li>
<!-- <li><p><b>Honors</b>: “Merit Student” and “Excellent League Member” Awarded by SCUEC <br /></p>
</li> -->
</ul>
</td></tr></table>

<h2>Working Experiences</h2>
<!-- <p>Currently I am a third-year M.Sc. student in the School of Artificial Intelligence, BUPT.<br /></p>
<p>Before that, I got my B.Sc. degree (ranked top 7 out of 60 students) in the School of Automation in June 2019 from
BUPT</a>. In the same year, I was admitted to study for a M.Sc degree in BUPT without entrance examination.</p> -->

<table class="imgtable"><tr><td>
<a href="https://www.xiaohongshu.com/"><img src="xiaohongshu.png" alt="Xiaohongshu" width="80px" /></a>&nbsp;</td>
<td align="left"><h3>Intern, <a href="https://www.xiaohongshu.com/">Xiao hongshu</a> [2022.9 ~ ]</h3>
<ul>
<!-- <li><p>List of “Double First-Class” University Project、Project 985、Project 211</p>
</li> -->
<li><p><b>Department:</b> Multimodal Group </p>
</li>
<li><p><b>Content:</b> 3D Human Pose Generation </p>
</li>
<li><p><b>Mentor:</b> Haofan Wang <a href="https://scholar.google.com/citations?user=EaMsuB0AAAAJ&hl=en">[Google Scholar]</a> </p>
</li>
</ul>
</td></tr></table>


<table class="imgtable"><tr><td>
<a href="https://www.sensetime.com/en/"><img src="sensetime.png" alt="Sensetime" width="80px" /></a>&nbsp;</td>
<td align="left"><h3>Intern, <a href="https://www.sensetime.com/en/">Sensetime Group Limited </a> [2021.11 ~ 2022.3]</h3>
<ul>
<!-- <li><p>List of “Double First-Class” University Project、Project 985、Project 211</p>
</li> -->
<li><p><b>Department:</b> Urban Computing Group </p>
</li>
<li><p><b>Content:</b> 3D Human Reconstruction </p>
</li>
<li><p><b>Mentor:</b> Dongliang Wang <a href="https://scholar.google.com/citations?user=gurERzcAAAAJ&hl=en">[Google Scholar]</a> </p>
</li>
</ul>
</td></tr></table>

<table class="imgtable"><tr><td>
<a href="https://research.samsung.com/"><img src="samsung.png" alt="SamSung" width="80px" /></a>&nbsp;</td>
<td align="left"><h3>Intern, <a href="https://research.samsung.com/">Samsung R&D Institute China-Beijing</a> [2021.8 ~ 2021.10]</h3>
<ul>
<!-- <li><p>List of “Double First-Class” University Project、Project 985、Project 211</p>
</li> -->
<li><p><b>Department:</b> Computer Vision Group </p>
</li>
<li><p><b>Content:</b> 3D Human Pose Estimation </p>
</li>
<li><p><b>Mentor:</b> Yuan Jiahui </p>
</li>
</ul>
</td></tr></table>


                                              


<h2>Publications</h2>

 <ul>
<li><p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9745623h"> Expressive whole-body human motion prediction
</a> <br />
<b>Pengxiang Ding</b>, Qiongjie Cui, Qiuhong Ke, Hongwei Yi, Haofan Wang, Debin Zhang. <br />
<i>International Conference on Computer Vision(ICCV), under review, 2023.</a></i></p>
</li>
</ul>

<ul>
<li><p> MGT-Twins: Explore the multi-grained trajectories combined with self-augmented skeleton twins for human motion prediction
<br />
<b>Pengxiang Ding</b>, Jin Liu, Junyin Wang, Jianqin Yin, Jianming Zhang. <br />
<i>IEEE Transactions on Multimedia(TMM), under review, 2023.</a></i></p>
</li>
</ul>


 <ul>
<li><p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9745623h"> Towards more realistic human motion prediction with attention to motion coordination
</a> <br />
<b>Pengxiang Ding</b>, Jianqin Yin. <br />
<i>IEEE Transactions on Circuits and Systems for Video Technology(TCSVT), published, 2022.</a></i></p>
</li>
</ul>



<ul>
<li><p><a href="https://arxiv.org/abs/2107.03575"> Uncertainty-aware Human Motion Prediction
</a> <br />
<b>Pengxiang Ding</b>, Jianqin Yin. <br />
<i>Arxiv, 2021.</a></i></p>
</li>
</ul>


<ul>
<li><p> Instance-incremental Graph Generation from Real-world Point Clouds via Normalizing Flows
<br />
Chao Qi, Jianqin Yin, Jinghang Xu, <b>Pengxiang Ding</b>. <br />
<i>IEEE Transactions on Circuits and Systems for Video Technology(TCSVT), under review, 2023.</a></i></p>
</li>
</ul>

<ul>
<li><p> DC-net: Dual-Consistency Semi-Supervised Learning for 3D Left Atrium Segmentation from MRI
</a> <br />
Junyin Wang, Xiaoli Liu, Jianqin Yin, <b>Pengxiang Ding.</b> <br />
<i> Biomedical Signal Processing and Control, published, 2022.</a></i></p>
</li>
</ul>


<ul>
<li><p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9186039"> TrajectoryCNN: a new spatio-temporal feature learning network for human motion prediction
</a> <br />
Xiaoli Liu, Jianqin Yin, Jin Liu, <b>Pengxiang Ding</b>, Jun Liu , Huaping Liu. <br />
<i>IEEE Transactions on Circuits and Systems for Video Technology(TCSVT), published, 2021.</a></i></p>
</li>
</ul>








<div id="footer">
<div id="footer-text">
<br>Page generated 2022-05-02, by <a href="https://Dingpx.github.io/">Pengxiang Ding</a>.
</div>
</div>
</div>
</body>
</html>
