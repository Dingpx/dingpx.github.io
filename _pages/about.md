---
permalink: /
title: "Pengxiang Ding 丁鹏翔"
excerpt: "Pengxiang Ding 丁鹏翔"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hi! I am Pengxiang Ding (丁鹏翔 in Chinese). I am a second-year Ph.D. student at [Zhejiang University](http://www.zju.edu.cn/), advised by Prof. [Donglin Wang](https://en.westlake.edu.cn/about/faculty/201912/t20191206_2513.shtml). Additionally, I am involved in a joint program with [Westlake University](https://www.westlake.edu.cn/) as a member of [Machine Intelligence Laboratory (MiLAB)](https://milab.westlake.edu.cn/). Prior to my Ph.D. career, I received my Msc. Degree from School of Artificial Intelligence, [Beijing University of Posts and Telecommunications](https://www.bupt.edu.cn/) in 2022.

<!-- I am <span style="color:red;"><b>seeking exciting postdoc opportunities at top-tier institutions after Ph.D. graduation (July 2024)</b></span>. Please feel free to drop me an <a href="mailto:siteng.huang@gmail.com" target="_blank">email</a> if you are interested! -->

<h2 id='research-interests'>Research Interests</h2>

Currently, My research has centered on **embodied ai**, including 

* **Vision-Language-Action models**: Foundation models for robots.
* **Efficient Learning**: Pratical acceleration paradigm for foundation models
* **Data Centric Optimization**: Enhance the data utilization of limited robot data


I am always looking for related collaborations, and some of them have produced top-level publications. If you are interested in communication, feel free to drop me an <a href="mailto:dingpx2015@gmail.com" target="_blank">email</a>.


<h2 id='news'>News</h2>

* **[Mar 04, 2025]** [PD-VLA](https://arxiv.org/abs/2503.02310) is available in arxiv.
* **[Feb 27, 2025]** [BC-IB](https://arxiv.org/pdf/2502.02853) is available in arxiv.
* **[Feb 21, 2025]** [OTPR](https://www.arxiv.org/pdf/2502.12631) is available in arxiv.
* **[Feb 21, 2025]** [Humanoid-VLA](https://arxiv.org/pdf/2502.14795) is available in arxiv.
* **[Jan 28, 2025]** Two papers [MoRE](https://arxiv.org/abs/2503.08007), [QUART-Online](https://arxiv.org/abs/2412.15576) got accepted for ICRA 2025.
* **[Jan 23, 2025]** Two papers [VLAS](https://openreview.net/pdf?id=K4FAFNRpko), [GEVRM](https://openreview.net/pdf?id=hPWWXpCaJ7) got accepted for ICLR 2025.
<!-- * **[Dec 25, 2024]** [QUART-Online](https://arxiv.org/abs/2412.15576) is available in arxiv.  -->
* **[Dec 12, 2024]** [SDP](https://arxiv.org/pdf/2412.09265) is available in arxiv.
* **[Dec 10, 2024]** [CARP](https://arxiv.org/abs/2412.06782) is available in arxiv.
* **[Dec 10, 2024]** One paper [Cobra](https://arxiv.org/abs/2403.14520) got accepted for AAAI 2025.
* **[Jul 31, 2024]** One paper DHRNet got accepted for KBS 2024.
* **[Jul 16, 2024]** One paper ProFD got accepted for ACMMM 2024.
* **[Jul 02, 2024]** Two papers QUAR-VLA, PiTe (Oral) got accepted for ECCV 2024.
* **[Jun 30, 2024]** One paper GeRM got accepted for IROS 2024.
* **[May 04, 2024]** One paper RL2AC got accepted for RSS 2024.
* **[Dec 9, 2023]** [EAI](https://arxiv.org/abs/2312.11972) got accepted for AAAI 2024.


**Hiring**: We are looking for **postdoctors, research assistants and visiting students for MiLAB in Westlake University**. More information about requirements can be found [here](https://milab.westlake.edu.cn/contact.html), and if you are still in school, being a visiting student is also welcome. Please send email to `mi_lab[AT]westlake.edu.cn` with your CV if you are interested. Specially, if you are interested in my research direction and would like to be my collaborator after coming, please specify in the email and also send a copy to me.
{: .notice--info}

<h2 id='publications'>Publications</h2>

<a href="https://scholar.google.com/citations?user=QyBSTzEAAAAJ" target="_blank"><img src="https://img.shields.io/badge/dynamic/json?label=Paper%20Citations&query=total_citations&url=https%3A%2F%2Fcse.bth.se%2F~fer%2Fgooglescholar-api%2Fgooglescholar.php%3Fuser%3DQyBSTzEAAAAJ&logo=googlescholar&style=social" alt="Google Scholar"></a>  †: Equal contribution

### Peer-reviewed Conference

<img src="https://img.shields.io/badge/ICRA-2025-blue?style=flat-square"> 
Han Zhao, Wenxuan Song, Donglin Wang, Xinyang Tong, <u>Pengxiang Ding</u>, Xuelian Cheng, Zongyuan Ge &quot;**MoRE: Unlocking Scalability in Reinforcement Learning for Quadruped Vision-Language-Action Models**&quot;. [[paper](https://arxiv.org/abs/2503.08007)]

<img src="https://img.shields.io/badge/ICRA-2025-blue?style=flat-square"> Xinyang Tong†, <u>Pengxiang Ding†</u>, Donglin Wang, Wenjie Zhang, Can Cui, Mingyang Sun, Yiguo Fan, Han Zhao, Hongyin Zhang, Yonghao Dang, Siteng Huang, Shangke Lyu &quot;**QUART-Online: Latency-Free Large Multimodal Language Model for Quadruped Robot Learning**&quot;. [[paper](https://arxiv.org/abs/2412.15576)][[Project](https://quart-online.github.io/)]

<img src="https://img.shields.io/badge/ICLR-2025-blue?style=flat-square"> Hongyin Zhang, <u>Pengxiang Ding</u>, Shangke Lyu, Ying Peng, Donglin Wang, &quot;**GEVRM: Goal-Expressive Video Generation Model For Robust Visual Manipulation**&quot;. *The Thirteenth International Conference on Learning Representations (ICLR2025)*. [[paper](https://openreview.net/pdf?id=hPWWXpCaJ7)] 

<img src="https://img.shields.io/badge/ICLR-2025-blue?style=flat-square"> Wei Zhao, <u>Pengxiang Ding</u>, Zhang Min, Zhefei Gong, Shuanghao Bai, Han Zhao, Donglin Wang, &quot;**VLAS: Vision-Language-Action Model with Speech Instructions for Customized Robot Manipulation**&quot;. *The Thirteenth International Conference on Learning Representations (ICLR2025)*. [[paper](https://openreview.net/pdf?id=K4FAFNRpko)] 

<img src="https://img.shields.io/badge/AAAI-2025-blue?style=flat-square"> Han Zhao, Min Zhang, Wei Zhao, <u>Pengxiang Ding</u>, Siteng Huang, Donglin Wang, &quot;**Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference**&quot;. *AAAI2025*. [[paper](https://arxiv.org/pdf/2403.14520.pdf)] [[project page](https://sites.google.com/view/cobravlm)] [[Chinese intro](https://zhuanlan.zhihu.com/p/688544752)] [[github](https://github.com/h-zhao1997/cobra)] [[demo](https://huggingface.co/spaces/han1997/cobra)] [[Twitter@AK](https://twitter.com/_akhaliq/status/1771033002748837953?t=6S4PVZXg6GcXqi_-PFzipw&s=19)]

<img src="https://img.shields.io/badge/KBS%20-%202024%20-%20red"> Yonghao Dang, Jianqin Yin, Liyuan Liu, <u>Pengxiang Ding</u>, Yuan Sun, Yanzhu Hu, &quot;**DHRNet: A Dual-path Hierarchical Relation Network for multi-person pose estimation**&quot;. *Knowledge-Based Systems 2024 (KBS2024)*. [[paper](https://www.sciencedirect.com/science/article/pii/S0950705124008979)][[code](https://github.com/YHDang/dhrnet-multi-pose-estimation)]

<img src="https://img.shields.io/badge/ACMMM-2024-blue?style=flat-square"> Can Cui, Siteng Huang, Wenxuan Song, <u>Pengxiang Ding</u>, Min Zhang, Donglin Wang, &quot;**ProFD: Prompt-Guided Feature Disentangling for Occluded Person Re-Identification**&quot;. *ACM Multimedia 2024 (ACMMM24)*. [[paper](https://openreview.net/forum?id=o2axlPlXYY)][[code](https://github.com/Cuixxx/ProFD)]

<img src="https://img.shields.io/badge/ECCV-2024-blue?style=flat-square"> <u>Pengxiang Ding</u>, Han Zhao, Wenxuan Song, Wenjie Zhang, Min Zhang, Siteng Huang, Ningxi Yang, Donglin Wang, &quot;**QUAR-VLA: Vision-Language-Action Model for Quadruped Robots**&quot;. *The 18th European Conference on Computer Vision (ECCV2024)*. [[paper](https://arxiv.org/abs/2312.14457)] [[Project](https://sites.google.com/view/quar-vla/quar-vla-eccv24)]

<img src="https://img.shields.io/badge/ECCV-2024-blue?style=flat-square"> Yang Liu†, <u>Pengxiang Ding†</u>, Siteng Huang, Min Zhang, Han Zhao, Donglin Wang, &quot;**PiTe: Pixel-Temporal Alignment for Large Video-Language Model**&quot;. *The 18th European Conference on Computer Vision (ECCV2024)*.[[paper](https://arxiv.org/abs/2409.07239)][[code](https://github.com/yliu-cs/pite)]

<img src="https://img.shields.io/badge/IROS-2024-blue?style=flat-square"> Wenxuan Song, Han Zhao, <u>Pengxiang Ding</u>, Can Cui, Shangke Lyu, Yaning Fan, Donglin Wang, &quot;**GeRM: A Generalist Robotic Model with Mixture-of-experts for Quadruped Robot**&quot;. *IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS2024)*. [[paper](https://arxiv.org/abs/2403.13358)]

<img src="https://img.shields.io/badge/RSS-2024-blue?style=flat-square"> Shangke Lyu, Xin Lang, Han Zhao, Hongyin Zhang, <u>Pengxiang Ding</u>, Donglin Wang, &quot;**RL2AC: Reinforcement Learning-based Rapid Online Adaptive Control for Legged Robot Robust Locomotion**&quot;. *Robotics: Science and Systems 2024 (RSS24)*. 

<img src="https://img.shields.io/badge/AAAI-2024-blue?style=flat-square"> <u>Pengxiang Ding</u>, Qiongjie Cui, Min Zhang, Mengyuan Liu, Haofan Wang, Donglin Wang, &quot;**Expressive Forecasting of 3D Whole-body Human Motions**&quot;. In *Proceedings of the 38th AAAI Conference on Artificial Intelligence*. [[paper](https://arxiv.org/abs/2312.11972)]
[[code](https://github.com/Dingpx/EAI)] 

<img src="https://img.shields.io/badge/TCSVT%20-%202023%20-%20red"> Chao Qi, Jianqin Yin, Jinghang Xu, <u>Pengxiang Ding</u>, &quot;**Instance-incremental Scene Graph Generation from Real-world Point Clouds via Normalizing Flows**&quot;. In *IEEE Transactions on Circuits and Systems for Video Technology*. [[paper](https://arxiv.org/abs/2302.10425)][[code](https://github.com/chaoqi7/GPL3D)] 

<img src="https://img.shields.io/badge/TCSVT%20-%202022%20-%20red"> <u>Pengxiang Ding</u>, Jianqin Yin, &quot;**Towards more realistic human motion prediction with attention to motion coordination**&quot;. In *IEEE Transactions on Circuits and Systems for Video Technology*. [[paper](https://arxiv.org/abs/2404.03584)][[code](https://github.com/Dingpx/Motion-Coordination)] 

<img src="https://img.shields.io/badge/TCSVT%20-%202020%20-%20red"> Xiaoli Liu, Jianqin Yin, Jin Liu, <u>Pengxiang Ding</u>, Jun Liu, Huaping Liu, &quot;**Trajectorycnn: a new spatio-temporal feature learning network for human motion prediction**&quot;. In *IEEE Transactions on Circuits and Systems for Video Technology*. [[paper](https://arxiv.org/abs/1910.06583)][[code](https://github.com/lily2lab/TrajectoryCNN)]  

<h2 id='experience'>Experience</h2>

* Research Intern - **DAMO Academy, Machine Intelligence Laboratory (达摩院/机器智能实验室)**
  * Advisor: [Xin Li](https://scholar.google.com/citations?user=syD9lxQAAAAJ&hl=zh-CN)
  * Time: Jan 2025 - March 2025.

* Research Intern - **RedBook, Intelligent Creation Group (小红书/智能创作组)**
  * Advisor: [Haofan Wang](https://scholar.google.com/citations?user=EaMsuB0AAAAJ)
  * Time: Sep 2022 - March 2023.

* Research Intern - **Sensetime/Smart City Group (商汤/智慧城市事业群)**
  * Advisor: [Dongliang Wang](https://scholar.google.com/citations?user=gurERzcAAAAJ)
  * Time: Sep 2021 - March 2022.

<h2 id='services'>Services</h2>
### Journal/Conference Reviewer
* ICML, ICLR, NeurIPS
* CVPR, ICCV, ACMMM, ICME
* ICRA, IROS
* TNNLS, TASE, TSCVT

### Talk
* [End-to-End Quadruped Robot Large Model(端到端四足机器人大模型)](https://www.shenlanxueyuan.com/open/course/260)，深蓝学院

<h2 id='misc'>Misc</h2>
Welcome to follow my [Redbook](https://www.xiaohongshu.com/user/profile/5f66a898000000000100a687).



