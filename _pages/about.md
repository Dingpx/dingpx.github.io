---
permalink: /
title: "Pengxiang Ding 丁鹏翔"
excerpt: "Pengxiang Ding 丁鹏翔"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hi! I am Pengxiang Ding (丁鹏翔 in Chinese). I am a third-year Ph.D. student at [Zhejiang University](http://www.zju.edu.cn/), advised by Prof. [Donglin Wang](https://en.westlake.edu.cn/about/faculty/201912/t20191206_2513.shtml). Additionally, I am involved in a joint program with [Westlake University](https://www.westlake.edu.cn/) as a member of [Machine Intelligence Laboratory (MiLAB)](https://milab.westlake.edu.cn/). Prior to my Ph.D. career, I received my Msc. Degree from School of Artificial Intelligence, [Beijing University of Posts and Telecommunications](https://www.bupt.edu.cn/) in 2022, advised by Prof. [Jianqin Yin](https://teacher.bupt.edu.cn/yinjianqin/zh_CN/index.htm).

<!-- I am <span style="color:red;"><b>seeking exciting postdoc opportunities at top-tier institutions after Ph.D. graduation (July 2024)</b></span>. Please feel free to drop me an <a href="mailto:siteng.huang@gmail.com" target="_blank">email</a> if you are interested! -->

<h2 id='research-interests'>Research Interests</h2>

My research has centered on **embodied ai**, including VLA/VLM/World Model. During my Ph.D. studies, I primarily focused on the VLA direction. As first author, co-first author, or project leader, I published 15 papers, with representative works including Quar-VLA, Humanoid-VLA, OpenHelix, and CARP.



<h2 id='publications'> Major Publications</h2>


<a href="https://scholar.google.com/citations?user=QyBSTzEAAAAJ" target="_blank"><img src="https://img.shields.io/badge/dynamic/json?label=Paper%20Citations&query=total_citations&url=https%3A%2F%2Fcse.bth.se%2F~fer%2Fgooglescholar-api%2Fgooglescholar.php%3Fuser%3DQyBSTzEAAAAJ&logo=googlescholar&style=social" alt="Google Scholar"></a>  †: Equal contribution or Project Leader 

<img src="https://img.shields.io/badge/CoRL-2025-blue?style=flat-square">  Yiguo Fan†, <u>Pengxiang Ding†</u>,Shuanghao Bai†, Xinyang Tong†, Yuyang Zhu, Hongchao Lu, Fengqi Dai, Wei Zhao, Yang Liu, Siteng Huang, Zhaoxin Fan, Badong Chen, Donglin Wang. &quot;**Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation**&quot;. 



<img src="https://img.shields.io/badge/ICCV-2025-blue?style=flat-square">  Zhefei Gong†, <u>Pengxiang Ding†</u>, Shangke Lyu, Siteng Huang, Mingyang Sun, Wei Zhao, Zhaoxin Fan, Donglin Wang. &quot;**CARP: Visuomotor Policy Learning via Coarse-to-Fine Autoregressive Prediction**&quot;. 
[[paper](https://arxiv.org/pdf/2506.13725)]
[[webpage](https://carp-robot.github.io/)]
[[code](https://github.com/ZhefeiGong/carp)]




<img src="https://img.shields.io/badge/Arxiv-2025-blue?style=flat-square">  Wenxuan Song†, Jiayi Chen†, <u>Pengxiang Ding†</u>, Yuxin Huang, Han Zhao, Donglin Wang, Haoang Li. &quot;**CEED-VLA: Consistency Vision-Language-Action Model with Early-Exit Decoding**&quot;. 
[[paper](https://arxiv.org/pdf/2506.13725)]
[[webpage](https://irpn-eai.github.io/CEED-VLA/)]
[[code](https://github.com/OpenHelix-Team/CEED-VLA)]

<img src="https://img.shields.io/badge/Arxiv-2025-blue?style=flat-square">  Yang Liu†, Ming Ma†, Xiaomin Yu†, <u>Pengxiang Ding†</u>, Han Zhao, Mingyang Sun, Siteng Huang, Donglin Wang. &quot;**SSR: Enhancing Depth Perception in Vision-Language Models via Rationale-Guided Spatial Reasoning**&quot;. 
[[paper](https://arxiv.org/pdf/2505.12448?)]
[[webpage](https://yliu-cs.github.io/SSR/)]
[[code](https://github.com/yliu-cs/SSR)]


<img src="https://img.shields.io/badge/Arxiv-2025-blue?style=flat-square">  Xiaomin Yu†, <u>Pengxiang Ding†</u>, Wenjie Zhang, Siteng Huang, Songyang Gao, Chengwei Qin, Kejian Wu, Zhaoxin Fan, Ziyue Qiao, Donglin Wang. &quot;**Unicorn: Text-Only Data Synthesis for Vision Language Model Training**&quot;. 
[[paper](https://arxiv.org/abs/2503.22655)]
[[code](https://github.com/Yu-xm/Unicorn)]

<img src="https://img.shields.io/badge/Arxiv-2025-blue?style=flat-square"> Can Cui†, <u>Pengxiang Ding†</u>, Wenxuan Song, Shuanghao Bai, Xinyang Tong, Zirui Ge, Runze Suo, Wanqi Zhou, Yang Liu, Bofang Jia, Han Zhao, Siteng Huang, Donglin Wang. &quot;**Openhelix: A short survey, empirical analysis, and open-source dual-system vla model for robotic manipulation**&quot;. 
[[paper](https://arxiv.org/abs/2505.03912)]
[[webpage](https://openhelix-robot.github.io/)]
[[code](https://github.com/OpenHelix-robot/OpenHelix)]

<img src="https://img.shields.io/badge/Arxiv-2025-blue?style=flat-square"> <u>Pengxiang Ding†</u>, Jianfei Ma†, Xinyang Tong†, Binghong Zou, Xinxin Luo, Yiguo Fan, Ting Wang, Hongchao Lu, Panzhong Mo, Jinxin Liu, Yuefan Wang, Huaicheng Zhou, Wenshuo Feng, Jiacheng Liu, Siteng Huang, Donglin Wang. &quot;**Humanoid-vla: Towards universal humanoid control with visual integration**&quot;. 
[[paper](https://arxiv.org/pdf/2502.14795?)]



<img src="https://img.shields.io/badge/IROS-2025-blue?style=flat-square">  Wenxuan Song†, Jiayi Chen†, <u>Pengxiang Ding†</u>, Han Zhao, Wei Zhao, Zhide Zhong, Zongyuan Ge, Jun Ma, Haoang Li. &quot;**Accelerating vision-language-action model integrated with action chunking via parallel decoding**&quot;. 
[[paper](https://arxiv.org/abs/2503.22655)]

<img src="https://img.shields.io/badge/ICML-2025-blue?style=flat-square">   Shuanghao Bai†, Wanqi Zhou†, <u>Pengxiang Ding†</u>, Wei Zhao, Donglin Wang, Badong Chen. &quot;**Rethinking Latent Representations in Behavior Cloning:An Information Bottleneck Approach for Robot Manipulation**&quot;. 
[[paper](https://arxiv.org/pdf/2502.02853)]
[[webpage](https://baishuanghao.github.io/BC-IB.github.io/)]
[[code](https://github.com/BaiShuanghao/BC-IB)]


<img src="https://img.shields.io/badge/ICRA-2025-blue?style=flat-square"> Xinyang Tong†, <u>Pengxiang Ding†</u>, Donglin Wang, Wenjie Zhang, Can Cui, Mingyang Sun, Yiguo Fan, Han Zhao, Hongyin Zhang, Yonghao Dang, Siteng Huang, Shangke Lyu &quot;**QUART-Online: Latency-Free Large Multimodal Language Model for Quadruped Robot Learning**&quot;. 
[[paper](https://arxiv.org/abs/2412.15576)]
[[webpage](https://quart-online.github.io/)]

<img src="https://img.shields.io/badge/ICLR-2025-blue?style=flat-square"> Hongyin Zhang, <u>Pengxiang Ding†</u>, Shangke Lyu, Ying Peng, Donglin Wang, &quot;**GEVRM: Goal-Expressive Video Generation Model For Robust Visual Manipulation**&quot;. *The Thirteenth International Conference on Learning Representations (ICLR2025)*. [[paper](https://openreview.net/pdf?id=hPWWXpCaJ7)] 

<img src="https://img.shields.io/badge/ICLR-2025-blue?style=flat-square"> Wei Zhao, <u>Pengxiang Ding†</u>, Zhang Min, Zhefei Gong, Shuanghao Bai, Han Zhao, Donglin Wang, &quot;**VLAS: Vision-Language-Action Model with Speech Instructions for Customized Robot Manipulation**&quot;. *The Thirteenth International Conference on Learning Representations (ICLR2025)*. [[paper](https://openreview.net/pdf?id=K4FAFNRpko)] 

<img src="https://img.shields.io/badge/ECCV-2024-blue?style=flat-square"> <u>Pengxiang Ding</u>, Han Zhao, Wenxuan Song, Wenjie Zhang, Min Zhang, Siteng Huang, Ningxi Yang, Donglin Wang, &quot;**QUAR-VLA: Vision-Language-Action Model for Quadruped Robots**&quot;. *The 18th European Conference on Computer Vision (ECCV2024)*. [[paper](https://arxiv.org/abs/2312.14457)] [[Project](https://sites.google.com/view/quar-vla/quar-vla-eccv24)]

<img src="https://img.shields.io/badge/ECCV-2024-blue?style=flat-square"> Yang Liu†, <u>Pengxiang Ding†</u>, Siteng Huang, Min Zhang, Han Zhao, Donglin Wang, &quot;**PiTe: Pixel-Temporal Alignment for Large Video-Language Model**&quot;. *The 18th European Conference on Computer Vision (ECCV2024)*.[[paper](https://arxiv.org/abs/2409.07239)][[code](https://github.com/yliu-cs/pite)]



<img src="https://img.shields.io/badge/AAAI-2024-blue?style=flat-square"> <u>Pengxiang Ding</u>, Qiongjie Cui, Min Zhang, Mengyuan Liu, Haofan Wang, Donglin Wang, &quot;**Expressive Forecasting of 3D Whole-body Human Motions**&quot;. In *Proceedings of the 38th AAAI Conference on Artificial Intelligence*. [[paper](https://arxiv.org/abs/2312.11972)]
[[code](https://github.com/Dingpx/EAI)] 

<img src="https://img.shields.io/badge/TCSVT%20-%202022%20-%20red"> <u>Pengxiang Ding</u>, Jianqin Yin, &quot;**Towards more realistic human motion prediction with attention to motion coordination**&quot;. In *IEEE Transactions on Circuits and Systems for Video Technology*. [[paper](https://arxiv.org/abs/2404.03584)][[code](https://github.com/Dingpx/Motion-Coordination)] 


### Other Pubulications

<img src="https://img.shields.io/badge/ICML-2025-blue?style=flat-square">  Hongyin Zhang, Zifeng Zhuang, Han Zhao, <u>Pengxiang Ding</u>, Hongchao Lu, Donglin Wang.
&quot;**ReinboT: Amplifying Robot Visual-Language Manipulation with Reinforcement Learning.**&quot;



<img src="https://img.shields.io/badge/ICML-2025-blue?style=flat-square"> 
Mingyang Sun, <u>Pengxiang Ding</u>, Weinan Zhang, Donglin Wang.

&quot;**Score-Based Diffusion Policy Compatible with Reinforcement Learning via Optimal Transport.**&quot;
[[paper](https://arxiv.org/pdf/2502.02853)]

<img src="https://img.shields.io/badge/ICRA-2025-blue?style=flat-square"> 
Han Zhao, Wenxuan Song, Donglin Wang, Xinyang Tong, <u>Pengxiang Ding</u>, Xuelian Cheng, Zongyuan Ge.

&quot;**MoRE: Unlocking Scalability in Reinforcement Learning for Quadruped Vision-Language-Action Models**&quot;. [[paper](https://arxiv.org/abs/2503.08007)]

<img src="https://img.shields.io/badge/AAAI-2025-blue?style=flat-square"> Han Zhao, Min Zhang, Wei Zhao, <u>Pengxiang Ding</u>, Siteng Huang, Donglin Wang, &quot;**Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference**&quot;. *AAAI2025*. [[paper](https://arxiv.org/pdf/2403.14520.pdf)] [[project page](https://sites.google.com/view/cobravlm)] [[Chinese intro](https://zhuanlan.zhihu.com/p/688544752)] [[github](https://github.com/h-zhao1997/cobra)] [[demo](https://huggingface.co/spaces/han1997/cobra)] [[Twitter@AK](https://twitter.com/_akhaliq/status/1771033002748837953?t=6S4PVZXg6GcXqi_-PFzipw&s=19)]

<img src="https://img.shields.io/badge/KBS%20-%202024%20-%20red"> Yonghao Dang, Jianqin Yin, Liyuan Liu, <u>Pengxiang Ding</u>, Yuan Sun, Yanzhu Hu, &quot;**DHRNet: A Dual-path Hierarchical Relation Network for multi-person pose estimation**&quot;. *Knowledge-Based Systems 2024 (KBS2024)*. [[paper](https://www.sciencedirect.com/science/article/pii/S0950705124008979)][[code](https://github.com/YHDang/dhrnet-multi-pose-estimation)]

<img src="https://img.shields.io/badge/ACMMM-2024-blue?style=flat-square"> Can Cui, Siteng Huang, Wenxuan Song, <u>Pengxiang Ding</u>, Min Zhang, Donglin Wang, &quot;**ProFD: Prompt-Guided Feature Disentangling for Occluded Person Re-Identification**&quot;. *ACM Multimedia 2024 (ACMMM24)*. [[paper](https://openreview.net/forum?id=o2axlPlXYY)][[code](https://github.com/Cuixxx/ProFD)]


<img src="https://img.shields.io/badge/IROS-2024-blue?style=flat-square"> Wenxuan Song, Han Zhao, <u>Pengxiang Ding</u>, Can Cui, Shangke Lyu, Yaning Fan, Donglin Wang, &quot;**GeRM: A Generalist Robotic Model with Mixture-of-experts for Quadruped Robot**&quot;. *IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS2024)*. [[paper](https://arxiv.org/abs/2403.13358)]

<img src="https://img.shields.io/badge/RSS-2024-blue?style=flat-square"> Shangke Lyu, Xin Lang, Han Zhao, Hongyin Zhang, <u>Pengxiang Ding</u>, Donglin Wang, &quot;**RL2AC: Reinforcement Learning-based Rapid Online Adaptive Control for Legged Robot Robust Locomotion**&quot;. *Robotics: Science and Systems 2024 (RSS24)*. 


<img src="https://img.shields.io/badge/TCSVT%20-%202023%20-%20red"> Chao Qi, Jianqin Yin, Jinghang Xu, <u>Pengxiang Ding</u>, &quot;**Instance-incremental Scene Graph Generation from Real-world Point Clouds via Normalizing Flows**&quot;. In *IEEE Transactions on Circuits and Systems for Video Technology*. [[paper](https://arxiv.org/abs/2302.10425)][[code](https://github.com/chaoqi7/GPL3D)] 



<img src="https://img.shields.io/badge/TCSVT%20-%202020%20-%20red"> Xiaoli Liu, Jianqin Yin, Jin Liu, <u>Pengxiang Ding</u>, Jun Liu, Huaping Liu, &quot;**Trajectorycnn: a new spatio-temporal feature learning network for human motion prediction**&quot;. In *IEEE Transactions on Circuits and Systems for Video Technology*. [[paper](https://arxiv.org/abs/1910.06583)][[code](https://github.com/lily2lab/TrajectoryCNN)]  

<h2 id='experience'>Experience</h2>

* Research Intern - **DAMO Academy, Machine Intelligence Laboratory (达摩院/机器智能实验室)**
  * Advisor: [Xin Li](https://scholar.google.com/citations?user=syD9lxQAAAAJ&hl=zh-CN)
  * Time: Jan 2025 - June 2025.

* Research Intern - **RedBook, Intelligent Creation Group (小红书/智能创作组)**
  * Advisor: [Haofan Wang](https://scholar.google.com/citations?user=EaMsuB0AAAAJ)
  * Time: Sep 2022 - March 2023.

* Research Intern - **Sensetime/Smart City Group (商汤/智慧城市事业群)**
  * Advisor: [Dongliang Wang](https://scholar.google.com/citations?user=gurERzcAAAAJ)
  * Time: Sep 2021 - March 2022.

<h2 id='services'>Services</h2>
### Journal/Conference Reviewer
* ICML, ICLR, NeurIPS, CVPR, ICCV, ACMMM, AAAI, ICRA, IROS, CoRL, TNNLS, TASE, TSCVT

### Talk
1. [Vision-Language-Action Models](https://www.shenlanxueyuan.com/open/course/260) (Hao Dong, Peking University)
2. [MiLab VLA 专场 ](https://www.bilibili.com/video/BV1n9MHzNEBy/?spm_id_from=333.788.recommend_more_video.9&vd_source=0b82dde50876e70cde1beb1198083d82) (3D 视觉工坊)
3. [End-to-End Quadruped Robot Large Model](https://www.shenlanxueyuan.com/open/course/260)(深蓝学院)


<h2 id='misc'>Misc</h2>
Welcome to follow my [Redbook](https://www.xiaohongshu.com/user/profile/5f66a898000000000100a687).



